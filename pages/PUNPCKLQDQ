-   Index
-   May 2019



PUNPCKLBW/PUNPCKLWD/PUNPCKLDQ/PUNPCKLQDQ — UNPACK LOW DATA


  Opcode/Instruction                                                          Op/En   64/32 bit Mode Support   CPUID Feature Flag   Description
  --------------------------------------------------------------------------- ------- ------------------------ -------------------- ---------------------------------------------------------------------------------------------------------------
  NP 0F 60 /_r_¹ PUNPCKLBW _mm, mm/m32_                                       A       V/V                      MMX                  Interleave low-order bytes from _mm_ and _mm/m32_ into _mm_.
  66 0F 60 /_r_ PUNPCKLBW _xmm1_, _xmm2/m128_                                 A       V/V                      SSE2                 Interleave low-order bytes from _xmm1_ and _xmm2/m128_ into _xmm1_.
  NP 0F 61 /_r_¹ PUNPCKLWD _mm, mm/m32_                                       A       V/V                      MMX                  Interleave low-order words from _mm_ and _mm/m32_ into _mm_.
  66 0F 61 /_r_ PUNPCKLWD _xmm1_, _xmm2/m128_                                 A       V/V                      SSE2                 Interleave low-order words from _xmm1_ and _xmm2/m128_ into _xmm1_.
  NP 0F 62 /_r_¹ PUNPCKLDQ _mm, mm/m32_                                       A       V/V                      MMX                  Interleave low-order doublewords from _mm_ and _mm/m32_ into _mm_.
  66 0F 62 /_r_ PUNPCKLDQ _xmm1_, _xmm2/m128_                                 A       V/V                      SSE2                 Interleave low-order doublewords from _xmm1_ and _xmm2/m128_ into xmm1.
  66 0F 6C /_r_ PUNPCKLQDQ _xmm1_, _xmm2/m128_                                A       V/V                      SSE2                 Interleave low-order quadword from _xmm1_ and _xmm2/m128_ into _xmm1_ register.
  VEX.128.66.0F.WIG 60/r VPUNPCKLBW _xmm1,xmm2, xmm3/m128_                    B       V/V                      AVX                  Interleave low-order bytes from _xmm2_ and _xmm3/m128_ into _xmm1_.
  VEX.128.66.0F.WIG 61/r VPUNPCKLWD _xmm1,xmm2, xmm3/m128_                    B       V/V                      AVX                  Interleave low-order words from _xmm2_ and _xmm3/m128_ into _xmm1_.
  VEX.128.66.0F.WIG 62/r VPUNPCKLDQ _xmm1, xmm2, xmm3/m128_                   B       V/V                      AVX                  Interleave low-order doublewords from _xmm2_ and _xmm3/m128_ into _xmm1_.
  VEX.128.66.0F.WIG 6C/r VPUNPCKLQDQ _xmm1, xmm2, xmm3/m128_                  B       V/V                      AVX                  Interleave low-order quadword from _xmm2_ and _xmm3/m128_ into _xmm1_ register.
  VEX.256.66.0F.WIG 60 /r VPUNPCKLBW _ymm1, ymm2, ymm3/m256_                  B       V/V                      AVX2                 Interleave low-order bytes from _ymm2_ and _ymm3/m256_ into _ymm1_ register.
  VEX.256.66.0F.WIG 61 /r VPUNPCKLWD _ymm1, ymm2, ymm3/m256_                  B       V/V                      AVX2                 Interleave low-order words from _ymm2_ and _ymm3/m256_ into _ymm1_ register.
  VEX.256.66.0F.WIG 62 /r VPUNPCKLDQ _ymm1, ymm2, ymm3/m256_                  B       V/V                      AVX2                 Interleave low-order doublewords from _ymm2_ and _ymm3/m256_ into _ymm1_ register.
  VEX.256.66.0F.WIG 6C /r VPUNPCKLQDQ _ymm1, ymm2, ymm3/m256_                 B       V/V                      AVX2                 Interleave low-order quadword from _ymm2_ and _ymm3/m256_ into _ymm1_ register.
  EVEX.128.66.0F.WIG 60 /r VPUNPCKLBW xmm1 {k1}{z}, xmm2, xmm3/m128           C       V/V                      AVX512VL AVX512BW    Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1.
  EVEX.128.66.0F.WIG 61 /r VPUNPCKLWD xmm1 {k1}{z}, xmm2, xmm3/m128           C       V/V                      AVX512VL AVX512BW    Interleave low-order words from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1.
  EVEX.128.66.0F.W0 62 /r VPUNPCKLDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst    D       V/V                      AVX512VL AVX512F     Interleave low-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register subject to write mask k1.
  EVEX.128.66.0F.W1 6C /r VPUNPCKLQDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst   D       V/V                      AVX512VL AVX512F     Interleave low-order quadword from zmm2 and zmm3/m512/m64bcst into zmm1 register subject to write mask k1.

  --------------------------------------------------------------------------- --- ----- ------------------- ---------------------------------------------------------------------------------------------------------------
  EVEX.256.66.0F.WIG 60 /r VPUNPCKLBW ymm1 {k1}{z}, ymm2, ymm3/m256           C   V/V   AVX512VL AVX512BW   Interleave low-order bytes from ymm2 and ymm3/m256 into ymm1 register subject to write mask k1.
  EVEX.256.66.0F.WIG 61 /r VPUNPCKLWD ymm1 {k1}{z}, ymm2, ymm3/m256           C   V/V   AVX512VL AVX512BW   Interleave low-order words from ymm2 and ymm3/m256 into ymm1 register subject to write mask k1.
  EVEX.256.66.0F.W0 62 /r VPUNPCKLDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst    D   V/V   AVX512VL AVX512F    Interleave low-order doublewords from ymm2 and ymm3/m256/m32bcst into ymm1 register subject to write mask k1.
  EVEX.256.66.0F.W1 6C /r VPUNPCKLQDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst   D   V/V   AVX512VL AVX512F    Interleave low-order quadword from ymm2 and ymm3/m256/m64bcst into ymm1 register subject to write mask k1.
  EVEX.512.66.0F.WIG 60/r VPUNPCKLBW zmm1 {k1}{z}, zmm2, zmm3/m512            C   V/V   AVX512BW            Interleave low-order bytes from zmm2 and zmm3/m512 into zmm1 register subject to write mask k1.
  EVEX.512.66.0F.WIG 61/r VPUNPCKLWD zmm1 {k1}{z}, zmm2, zmm3/m512            C   V/V   AVX512BW            Interleave low-order words from zmm2 and zmm3/m512 into zmm1 register subject to write mask k1.
  EVEX.512.66.0F.W0 62 /r VPUNPCKLDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst    D   V/V   AVX512F             Interleave low-order doublewords from zmm2 and zmm3/m512/m32bcst into zmm1 register subject to write mask k1.
  EVEX.512.66.0F.W1 6C /r VPUNPCKLQDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst   D   V/V   AVX512F             Interleave low-order quadword from zmm2 and zmm3/m512/m64bcst into zmm1 register subject to write mask k1.
  --------------------------------------------------------------------------- --- ----- ------------------- ---------------------------------------------------------------------------------------------------------------

  1. See note in Section 2.4, “AVX and SSE Instruction Exception
  Specification” in the _Intel® 64 and IA-32 Architectures Software
  Developer’s Manual, Volume 2A_ and Section 22.25.3, “Exception
  Conditions of Legacy SIMD Instructions Operating on MMX Registers” in
  the _Intel® 64 and IA-32 Architectures Software Developer’s Manual,
  Volume 3A_.


Instruction Operand Encoding ¶

  ------- ------------ ------------------ --------------- --------------- -----------
  Op/En   Tuple Type   Operand 1          Operand 2       Operand 3       Operand 4
  A       NA           ModRM:reg (r, w)   ModRM:r/m (r)   NA              NA
  B       NA           ModRM:reg (w)      VEX.vvvv (r)    ModRM:r/m (r)   NA
  C       Full Mem     ModRM:reg (w)      EVEX.vvvv (r)   ModRM:r/m (r)   NA
  D       Full         ModRM:reg (w)      EVEX.vvvv (r)   ModRM:r/m (r)   NA
  ------- ------------ ------------------ --------------- --------------- -----------


Description ¶

Unpacks and interleaves the low-order data elements (bytes, words,
doublewords, and quadwords) of the destination operand (first operand)
and source operand (second operand) into the destination operand.
(Figure 4-22 shows the unpack operation for bytes in 64-bit operands.).
The high-order data elements are ignored.

SRC X7 X6 X5 X4 X3 X2 X1 X0 DEST Y7 Y6 Y5 Y4 Y3 Y2 Y1 Y0 Y3 X3 Y2 X2 Y1
X1 Y0 X0 DEST

Figure 4-22. PUNPCKLBW Instruction Operation Using 64-bit Operands

255 ^(31\ 0) 255 31 0

SRC 255 0 DEST

Figure 4-23. 256-bit VPUNPCKLDQ Instruction Operation

When the source data comes from a 128-bit memory operand, an
implementation may fetch only the appropriate 64 bits; however,
alignment to a 16-byte boundary and normal segment checking will still
be enforced.

The (V)PUNPCKLBW instruction interleaves the low-order bytes of the
source and destination operands, the (V)PUNPCKLWD instruction
interleaves the low-order words of the source and destination operands,
the (V)PUNPCKLDQ instruction interleaves the low-order doubleword (or
doublewords) of the source and destination operands, and the
(V)PUNPCKLQDQ instruction interleaves the low-order quadwords of the
source and destination operands.

These instructions can be used to convert bytes to words, words to
doublewords, doublewords to quadwords, and quadwords to double
quadwords, respectively, by placing all 0s in the source operand. Here,
if the source operand contains all 0s, the result (stored in the
destination operand) contains zero extensions of the high-order data
elements from the original value in the destination operand. For
example, with the (V)PUNPCKLBW instruction the high-order bytes are zero
extended (that is, unpacked into unsigned word integers), and with the
(V)PUNPCKLWD instruction, the high-order words are zero extended
(unpacked into unsigned doubleword integers).

In 64-bit mode and not encoded with VEX/EVEX, using a REX prefix in the
form of REX.R permits this instruction to access additional registers
(XMM8-XMM15).

Legacy SSE versions 64-bit operand: The source operand can be an MMX
technology register or a 32-bit memory location. The destination operand
is an MMX technology register.

128-bit Legacy SSE versions: The second source operand is an XMM
register or a 128-bit memory location. The first source operand and
destination operands are XMM registers. Bits (MAXVL-1:128) of the
corresponding YMM destination register remain unchanged.

VEX.128 encoded versions: The second source operand is an XMM register
or a 128-bit memory location. The first source operand and destination
operands are XMM registers. Bits (MAXVL-1:128) of the destination YMM
register are zeroed.

VEX.256 encoded version: The second source operand is an YMM register or
an 256-bit memory location. The first source operand and destination
operands are YMM registers. Bits (MAXVL-1:256) of the corresponding ZMM
register are zeroed.

EVEX encoded VPUNPCKLDQ/QDQ: The second source operand is a ZMM/YMM/XMM
register, a 512/256/128-bit memory location or a 512/256/128-bit vector
broadcasted from a 32/64-bit memory location. The first source operand
and destination operands are ZMM/YMM/XMM registers. The destination is
conditionally updated with writemask k1.

EVEX encoded VPUNPCKLWD/BW: The second source operand is a ZMM/YMM/XMM
register, a 512/256/128-bit memory location. The first source operand
and destination operands are ZMM/YMM/XMM registers. The destination is
conditionally updated with writemask k1.


Operation ¶

PUNPCKLBW instruction with 64-bit operands: ¶

    DEST[63:56] ← SRC[31:24];
    DEST[55:48] ← DEST[31:24];
    DEST[47:40] ← SRC[23:16];
    DEST[39:32] ← DEST[23:16];
    DEST[31:24] ← SRC[15:8];
    DEST[23:16] ← DEST[15:8];
    DEST[15:8] ← SRC[7:0];
    DEST[7:0] ← DEST[7:0];

PUNPCKLWD instruction with 64-bit operands: ¶

    DEST[63:48] ← SRC[31:16];
    DEST[47:32] ← DEST[31:16];
    DEST[31:16] ← SRC[15:0];
    DEST[15:0] ← DEST[15:0];

PUNPCKLDQ instruction with 64-bit operands: ¶

        DEST[63:32] ← SRC[31:0];
        DEST[31:0] ← DEST[31:0];
    INTERLEAVE_BYTES_512b (SRC1, SRC2)
    TMP_DEST[255:0]←INTERLEAVE_BYTES_256b(SRC1[255:0], SRC[255:0])
    TMP_DEST[511:256]←INTERLEAVE_BYTES_256b(SRC1[511:256], SRC[511:256])
    INTERLEAVE_BYTES_256b (SRC1, SRC2)
    DEST[7:0] ← SRC1[7:0]
    DEST[15:8] ← SRC2[7:0]
    DEST[23:16] ← SRC1[15:8]
    DEST[31:24] ← SRC2[15:8]
    DEST[39:32] ← SRC1[23:16]
    DEST[47:40] ← SRC2[23:16]
    DEST[55:48] ← SRC1[31:24]
    DEST[63:56] ← SRC2[31:24]
    DEST[71:64] ← SRC1[39:32]
    DEST[79:72] ← SRC2[39:32]
    DEST[87:80] ← SRC1[47:40]
    DEST[95:88] ← SRC2[47:40]
    DEST[103:96] ← SRC1[55:48]
    DEST[111:104] ← SRC2[55:48]
    DEST[119:112] ← SRC1[63:56]
    DEST[127:120] ← SRC2[63:56]
    DEST[135:128] ← SRC1[135:128]
    DEST[143:136] ← SRC2[135:128]
    DEST[151:144] ← SRC1[143:136]
    DEST[159:152] ← SRC2[143:136]
    DEST[167:160] ← SRC1[151:144]
    DEST[175:168] ← SRC2[151:144]
    DEST[183:176] ← SRC1[159:152]
    DEST[191:184] ← SRC2[159:152]
    DEST[199:192] ← SRC1[167:160]
    DEST[207:200] ← SRC2[167:160]
    DEST[215:208] ← SRC1[175:168]
    DEST[223:216] ← SRC2[175:168]
    DEST[231:224] ← SRC1[183:176]
    DEST[239:232] ← SRC2[183:176]
    DEST[247:240] ← SRC1[191:184]
    DEST[255:248] ← SRC2[191:184]
    INTERLEAVE_BYTES (SRC1, SRC2)
    DEST[7:0] ← SRC1[7:0]
    DEST[15:8] ← SRC2[7:0]
    DEST[23:16] ← SRC2[15:8]
    DEST[31:24] ← SRC2[15:8]
    DEST[39:32] ← SRC1[23:16]
    DEST[47:40] ← SRC2[23:16]
    DEST[55:48] ← SRC1[31:24]
    DEST[63:56] ← SRC2[31:24]
    DEST[71:64] ← SRC1[39:32]
    DEST[79:72] ← SRC2[39:32]
    DEST[87:80] ← SRC1[47:40]
    DEST[95:88] ← SRC2[47:40]
    DEST[103:96] ← SRC1[55:48]
    DEST[111:104] ← SRC2[55:48]
    DEST[119:112] ← SRC1[63:56]
    DEST[127:120] ← SRC2[63:56]
    INTERLEAVE_WORDS_512b (SRC1, SRC2)
    TMP_DEST[255:0]←INTERLEAVE_WORDS_256b(SRC1[255:0], SRC[255:0])
    TMP_DEST[511:256]←INTERLEAVE_WORDS_256b(SRC1[511:256], SRC[511:256])
    INTERLEAVE_WORDS_256b(SRC1, SRC2)
    DEST[15:0] ← SRC1[15:0]
    DEST[31:16] ← SRC2[15:0]
    DEST[47:32] ← SRC1[31:16]
    DEST[63:48] ← SRC2[31:16]
    DEST[79:64] ← SRC1[47:32]
    DEST[95:80] ← SRC2[47:32]
    DEST[111:96] ← SRC1[63:48]
    DEST[127:112] ← SRC2[63:48]
    DEST[143:128] ← SRC1[143:128]
    DEST[159:144] ← SRC2[143:128]
    DEST[175:160] ← SRC1[159:144]
    DEST[191:176] ← SRC2[159:144]
    DEST[207:192] ← SRC1[175:160]
    DEST[223:208] ← SRC2[175:160]
    DEST[239:224] ← SRC1[191:176]
    DEST[255:240] ← SRC2[191:176]
    INTERLEAVE_WORDS (SRC1, SRC2)
    DEST[15:0] ← SRC1[15:0]
    DEST[31:16] ← SRC2[15:0]
    DEST[47:32] ← SRC1[31:16]
    DEST[63:48] ← SRC2[31:16]
    DEST[79:64] ← SRC1[47:32]
    DEST[95:80] ← SRC2[47:32]
    DEST[111:96] ← SRC1[63:48]
    DEST[127:112] ← SRC2[63:48]
    INTERLEAVE_DWORDS_512b (SRC1, SRC2)
    TMP_DEST[255:0]←INTERLEAVE_DWORDS_256b(SRC1[255:0], SRC2[255:0])
    TMP_DEST[511:256]←INTERLEAVE_DWORDS_256b(SRC1[511:256], SRC2[511:256])
    INTERLEAVE_DWORDS_256b(SRC1, SRC2)
    DEST[31:0] ← SRC1[31:0]
    DEST[63:32] ← SRC2[31:0]
    DEST[95:64] ← SRC1[63:32]
    DEST[127:96] ← SRC2[63:32]
    DEST[159:128] ← SRC1[159:128]
    DEST[191:160] ← SRC2[159:128]
    DEST[223:192] ← SRC1[191:160]
    DEST[255:224] ← SRC2[191:160]
    INTERLEAVE_DWORDS(SRC1, SRC2)
    DEST[31:0] ← SRC1[31:0]
    DEST[63:32] ← SRC2[31:0]
    DEST[95:64] ← SRC1[63:32]
    DEST[127:96] ← SRC2[63:32]
    INTERLEAVE_QWORDS_512b (SRC1, SRC2)
    TMP_DEST[255:0]←INTERLEAVE_QWORDS_256b(SRC1[255:0], SRC2[255:0])
    TMP_DEST[511:256]←INTERLEAVE_QWORDS_256b(SRC1[511:256], SRC2[511:256])
    INTERLEAVE_QWORDS_256b(SRC1, SRC2)
    DEST[63:0] ← SRC1[63:0]
    DEST[127:64] ← SRC2[63:0]
    DEST[191:128] ← SRC1[191:128]
    DEST[255:192] ← SRC2[191:128]
    INTERLEAVE_QWORDS(SRC1, SRC2)
    DEST[63:0] ← SRC1[63:0]
    DEST[127:64] ← SRC2[63:0]

PUNPCKLBW ¶

    DEST[127:0]←INTERLEAVE_BYTES(DEST, SRC)
    DEST[255:127] (Unmodified)

VPUNPCKLBW (VEX.128 encoded instruction) ¶

    DEST[127:0]←INTERLEAVE_BYTES(SRC1, SRC2)
    DEST[MAXVL-1:127] ←0

VPUNPCKLBW (VEX.256 encoded instruction) ¶

    DEST[255:0]←INTERLEAVE_BYTES_256b(SRC1, SRC2)
    DEST[MAXVL-1:256] ←0

VPUNPCKLBW (EVEX.512 encoded instruction) ¶

    (KL, VL) = (16, 128), (32, 256), (64, 512)
    IF VL = 128
        TMP_DEST[VL-1:0]←INTERLEAVE_BYTES(SRC1[VL-1:0], SRC2[VL-1:0])
    FI;
    IF VL = 256
        TMP_DEST[VL-1:0]←INTERLEAVE_BYTES_256b(SRC1[VL-1:0], SRC2[VL-1:0])
    FI;
    IF VL = 512
        TMP_DEST[VL-1:0]←INTERLEAVE_BYTES_512b(SRC1[VL-1:0], SRC2[VL-1:0])
    FI;
    FOR j←0 TO KL-1
        i←j * 8
        IF k1[j] OR *no writemask*
            THEN DEST[i+7:i]←TMP_DEST[i+7:i]
            ELSE
                IF *merging-masking*
                            ; merging-masking
                    THEN *DEST[i+7:i] remains unchanged*
                    ELSE *zeroing-masking*
                                ; zeroing-masking
                        DEST[i+7:i] ← 0
                FI
        FI;
    ENDFOR
    DEST[MAXVL-1:VL] ← 0
    DEST[511:0]←INTERLEAVE_BYTES_512b(SRC1, SRC2)

PUNPCKLWD ¶

    DEST[127:0]←INTERLEAVE_WORDS(DEST, SRC)
    DEST[255:127] (Unmodified)

VPUNPCKLWD (VEX.128 encoded instruction) ¶

    DEST[127:0]←INTERLEAVE_WORDS(SRC1, SRC2)
    DEST[MAXVL-1:127] ←0

VPUNPCKLWD (VEX.256 encoded instruction) ¶

    DEST[255:0]←INTERLEAVE_WORDS_256b(SRC1, SRC2)
    DEST[MAXVL-1:256] ←0

VPUNPCKLWD (EVEX.512 encoded instruction) ¶

    (KL, VL) = (8, 128), (16, 256), (32, 512)
    IF VL = 128
        TMP_DEST[VL-1:0]←INTERLEAVE_WORDS(SRC1[VL-1:0], SRC2[VL-1:0])
    FI;
    IF VL = 256
        TMP_DEST[VL-1:0]←INTERLEAVE_WORDS_256b(SRC1[VL-1:0], SRC2[VL-1:0])
    FI;
    IF VL = 512
        TMP_DEST[VL-1:0]←INTERLEAVE_WORDS_512b(SRC1[VL-1:0], SRC2[VL-1:0])
    FI;
    FOR j←0 TO KL-1
        i←j * 16
        IF k1[j] OR *no writemask*
            THEN DEST[i+15:i]←TMP_DEST[i+15:i]
            ELSE
                IF *merging-masking* ; merging-masking
                    THEN *DEST[i+15:i] remains unchanged*
                    ELSE *zeroing-masking*
                            ; zeroing-masking
                        DEST[i+15:i] ← 0
                FI
        FI;
    ENDFOR
    DEST[MAXVL-1:VL] ← 0
    DEST[511:0]←INTERLEAVE_WORDS_512b(SRC1, SRC2)

PUNPCKLDQ ¶

    DEST[127:0]←INTERLEAVE_DWORDS(DEST, SRC)
    DEST[MAXVL-1:128] (Unmodified)

VPUNPCKLDQ (VEX.128 encoded instruction) ¶

    DEST[127:0]←INTERLEAVE_DWORDS(SRC1, SRC2)
    DEST[MAXVL-1:128] ←0

VPUNPCKLDQ (VEX.256 encoded instruction) ¶

    DEST[255:0]←INTERLEAVE_DWORDS_256b(SRC1, SRC2)
    DEST[MAXVL-1:256] ←0

VPUNPCKLDQ (EVEX encoded instructions) ¶

    (KL, VL) = (4, 128), (8, 256), (16, 512)
    FOR j←0 TO KL-1
        i←j * 32
        IF (EVEX.b = 1) AND (SRC2 *is memory*)
            THEN TMP_SRC2[i+31:i]←SRC2[31:0]
            ELSE TMP_SRC2[i+31:i]←SRC2[i+31:i]
        FI;
    ENDFOR;
    IF VL = 128
        TMP_DEST[VL-1:0]←INTERLEAVE_DWORDS(SRC1[VL-1:0], TMP_SRC2[VL-1:0])
    FI;
    IF VL = 256
        TMP_DEST[VL-1:0]←INTERLEAVE_DWORDS_256b(SRC1[VL-1:0], TMP_SRC2[VL-1:0])
    FI;
    IF VL = 512
        TMP_DEST[VL-1:0]←INTERLEAVE_DWORDS_512b(SRC1[VL-1:0], TMP_SRC2[VL-1:0])
    FI;
    FOR j←0 TO KL-1
        i←j * 32
        IF k1[j] OR *no writemask*
            THEN DEST[i+31:i]←TMP_DEST[i+31:i]
            ELSE
                IF *merging-masking*
                            ; merging-masking
                    THEN *DEST[i+31:i] remains unchanged*
                    ELSE *zeroing-masking*
                                ; zeroing-masking
                        DEST[i+31:i] ← 0
                FI
        FI;
    ENDFOR
    DEST511:0]←INTERLEAVE_DWORDS_512b(SRC1, SRC2)
    DEST[MAXVL-1:VL] ← 0

PUNPCKLQDQ ¶

    DEST[127:0]←INTERLEAVE_QWORDS(DEST, SRC)
    DEST[MAXVL-1:128] (Unmodified)

VPUNPCKLQDQ (VEX.128 encoded instruction) ¶

    DEST[127:0]←INTERLEAVE_QWORDS(SRC1, SRC2)
    DEST[MAXVL-1:128] ←0

VPUNPCKLQDQ (VEX.256 encoded instruction) ¶

    DEST[255:0]←INTERLEAVE_QWORDS_256b(SRC1, SRC2)
    DEST[MAXVL-1:256] ←0

VPUNPCKLQDQ (EVEX encoded instructions) ¶

    (KL, VL) = (2, 128), (4, 256), (8, 512)
    FOR j←0 TO KL-1
        i←j * 64
        IF (EVEX.b = 1) AND (SRC2 *is memory*)
            THEN TMP_SRC2[i+63:i]←SRC2[63:0]
            ELSE TMP_SRC2[i+63:i]←SRC2[i+63:i]
        FI;
    ENDFOR;
    IF VL = 128
        TMP_DEST[VL-1:0]←INTERLEAVE_QWORDS(SRC1[VL-1:0], TMP_SRC2[VL-1:0])
    FI;
    IF VL = 256
        TMP_DEST[VL-1:0]←INTERLEAVE_QWORDS_256b(SRC1[VL-1:0], TMP_SRC2[VL-1:0])
    FI;
    IF VL = 512
        TMP_DEST[VL-1:0]←INTERLEAVE_QWORDS_512b(SRC1[VL-1:0], TMP_SRC2[VL-1:0])
    FI;
    FOR j←0 TO KL-1
        i←j * 64
        IF k1[j] OR *no writemask*
            THEN DEST[i+63:i]←TMP_DEST[i+63:i]
            ELSE
                IF *merging-masking*
                            ; merging-masking
                    THEN *DEST[i+63:i] remains unchanged*
                    ELSE *zeroing-masking*
                                ; zeroing-masking
                        DEST[i+63:i] ← 0
                FI
        FI;
    ENDFOR
    DEST[MAXVL-1:VL] ← 0


Intel C/C++ Compiler Intrinsic Equivalents ¶

    VPUNPCKLBW __m512i _mm512_unpacklo_epi8(__m512i a, __m512i b);

    VPUNPCKLBW __m512i _mm512_mask_unpacklo_epi8(__m512i s, __mmask64 k, __m512i a, __m512i b);

    VPUNPCKLBW __m512i _mm512_maskz_unpacklo_epi8( __mmask64 k, __m512i a, __m512i b);

    VPUNPCKLBW __m256i _mm256_mask_unpacklo_epi8(__m256i s, __mmask32 k, __m256i a, __m256i b);

    VPUNPCKLBW __m256i _mm256_maskz_unpacklo_epi8( __mmask32 k, __m256i a, __m256i b);

    VPUNPCKLBW __m128i _mm_mask_unpacklo_epi8(v s, __mmask16 k, __m128i a, __m128i b);

    VPUNPCKLBW __m128i _mm_maskz_unpacklo_epi8( __mmask16 k, __m128i a, __m128i b);

    VPUNPCKLWD __m512i _mm512_unpacklo_epi16(__m512i a, __m512i b);

    VPUNPCKLWD __m512i _mm512_mask_unpacklo_epi16(__m512i s, __mmask32 k, __m512i a, __m512i b);

    VPUNPCKLWD __m512i _mm512_maskz_unpacklo_epi16( __mmask32 k, __m512i a, __m512i b);

    VPUNPCKLWD __m256i _mm256_mask_unpacklo_epi16(__m256i s, __mmask16 k, __m256i a, __m256i b);

    VPUNPCKLWD __m256i _mm256_maskz_unpacklo_epi16( __mmask16 k, __m256i a, __m256i b);

    VPUNPCKLWD __m128i _mm_mask_unpacklo_epi16(v s, __mmask8 k, __m128i a, __m128i b);

    VPUNPCKLWD __m128i _mm_maskz_unpacklo_epi16( __mmask8 k, __m128i a, __m128i b);

    VPUNPCKLDQ __m512i _mm512_unpacklo_epi32(__m512i a, __m512i b);

    VPUNPCKLDQ __m512i _mm512_mask_unpacklo_epi32(__m512i s, __mmask16 k, __m512i a, __m512i b);

    VPUNPCKLDQ __m512i _mm512_maskz_unpacklo_epi32( __mmask16 k, __m512i a, __m512i b);

    VPUNPCKLDQ __m256i _mm256_mask_unpacklo_epi32(__m256i s, __mmask8 k, __m256i a, __m256i b);

    VPUNPCKLDQ __m256i _mm256_maskz_unpacklo_epi32( __mmask8 k, __m256i a, __m256i b);

    VPUNPCKLDQ __m128i _mm_mask_unpacklo_epi32(v s, __mmask8 k, __m128i a, __m128i b);

    VPUNPCKLDQ __m128i _mm_maskz_unpacklo_epi32( __mmask8 k, __m128i a, __m128i b);

    VPUNPCKLQDQ __m512i _mm512_unpacklo_epi64(__m512i a, __m512i b);

    VPUNPCKLQDQ __m512i _mm512_mask_unpacklo_epi64(__m512i s, __mmask8 k, __m512i a, __m512i b);

    VPUNPCKLQDQ __m512i _mm512_maskz_unpacklo_epi64( __mmask8 k, __m512i a, __m512i b);

    VPUNPCKLQDQ __m256i _mm256_mask_unpacklo_epi64(__m256i s, __mmask8 k, __m256i a, __m256i b);

    VPUNPCKLQDQ __m256i _mm256_maskz_unpacklo_epi64( __mmask8 k, __m256i a, __m256i b);

    VPUNPCKLQDQ __m128i _mm_mask_unpacklo_epi64(__m128i s, __mmask8 k, __m128i a, __m128i b);

    VPUNPCKLQDQ __m128i _mm_maskz_unpacklo_epi64( __mmask8 k, __m128i a, __m128i b);

    PUNPCKLBW:__m64 _mm_unpacklo_pi8 (__m64 m1, __m64 m2)

    (V)PUNPCKLBW:__m128i _mm_unpacklo_epi8 (__m128i m1, __m128i m2)

    VPUNPCKLBW:__m256i _mm256_unpacklo_epi8 (__m256i m1, __m256i m2)

    PUNPCKLWD:__m64 _mm_unpacklo_pi16 (__m64 m1, __m64 m2)

    (V)PUNPCKLWD:__m128i _mm_unpacklo_epi16 (__m128i m1, __m128i m2)

    VPUNPCKLWD:__m256i _mm256_unpacklo_epi16 (__m256i m1, __m256i m2)

    PUNPCKLDQ:__m64 _mm_unpacklo_pi32 (__m64 m1, __m64 m2)

    (V)PUNPCKLDQ:__m128i _mm_unpacklo_epi32 (__m128i m1, __m128i m2)

    VPUNPCKLDQ:__m256i _mm256_unpacklo_epi32 (__m256i m1, __m256i m2)

    (V)PUNPCKLQDQ:__m128i _mm_unpacklo_epi64 (__m128i m1, __m128i m2)

    VPUNPCKLQDQ:__m256i _mm256_unpacklo_epi64 (__m256i m1, __m256i m2)


Flags Affected ¶

None.


Numeric Exceptions ¶

None.


Other Exceptions ¶

Non-EVEX-encoded instruction, see Exceptions Type 4.

EVEX-encoded VPUNPCKLDQ/QDQ, see Exceptions Type E4NF.

EVEX-encoded VPUNPCKLBW/WD, see Exceptions Type E4NF.nb.

This UNOFFICIAL, mechanically-separated, non-verified reference is
provided for convenience, but it may be incomplete or b_(r)oke_(n) in
various obvious or non-obvious ways. Refer to Intel® 64 and IA-32
Architectures Software Developer’s Manual for anything serious.
